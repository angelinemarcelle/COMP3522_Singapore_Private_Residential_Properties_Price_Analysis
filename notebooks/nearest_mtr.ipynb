{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c406e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/angelinemarcellelukito/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/angelinemarcellelukito/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdb4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b17bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(project_name):\n",
    "    \"\"\"Get coordinates from OneMap API for a given project name\"\"\"\n",
    "    base_url = \"https://www.onemap.gov.sg/api/common/elastic/search\"\n",
    "    headers = {\"Authorization\": \"Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4OTgyLCJmb3JldmVyIjpmYWxzZSwiaXNzIjoiT25lTWFwIiwiaWF0IjoxNzU4NzA5MDY0LCJuYmYiOjE3NTg3MDkwNjQsImV4cCI6MTc1ODk2ODI2NCwianRpIjoiZDJiMGI0ZDktZjUwMi00NzlkLTg1MGQtNjMxZThkNTg1YWE0In0.Zx3mwVumXn7b06tPDpViZEwM_UPV3vH57T_F85v0RZL9bU3Pkr1SeHp2U2E0mgzWeRSO5e-lfT2PmHvw5Abn8E-X3V0brG5Ke9QJNjsaaKocOQuTXKoabS4_X2-GN7GkGPPr5-IFR5braFhTHzZfFmC2vwDwEP6IDYkURV8NuzjmT8yLX29l_gVkiQPxI1_3MPYahDT0sb1IXTRjAmP6R6RGVtVfnQTI1splQxcfNZguY3u4l441caafnoJo101kcFaXLAKo4d2V0EqoN1aKph92wHjbIkFioF-0d_8JGLgshMuHSm2KsI1IpruMnR-x7M1bKkYFSybszg7KuszN_A\"}\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Clean project name\n",
    "        project_name = str(project_name).strip()\n",
    "        \n",
    "        # Parameters for the API call\n",
    "        params = {\n",
    "            'searchVal': project_name,\n",
    "            'returnGeom': 'Y',\n",
    "            'getAddrDetails': 'Y'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data['found'] > 0:\n",
    "                result = data['results'][0]\n",
    "                return float(result['LATITUDE']), float(result['LONGITUDE']), str(result['ADDRESS'])\n",
    "        \n",
    "        return None, None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {project_name}: {str(e)}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates for each historical data district dataset\n",
    "for district_num in range(25, 29):\n",
    "    # Read the existing dataset\n",
    "    df = pd.read_csv(f'../datasets/historical_datasets/district{district_num}.csv', encoding='latin1')\n",
    "\n",
    "    # Create new columns for coordinates\n",
    "    df['Latitude'] = None\n",
    "    df['Longitude'] = None\n",
    "    df['Full Address'] = None\n",
    "\n",
    "    # Process each unique project name\n",
    "    for project_name in df['Project Name'].unique():\n",
    "        print(f\"Processing: {project_name}\")\n",
    "        lat, lon, address = get_coordinates(project_name)\n",
    "        \n",
    "        # Update all rows with this project name\n",
    "        if lat is not None and lon is not None:\n",
    "            df.loc[df['Project Name'] == project_name, 'Latitude'] = lat\n",
    "            df.loc[df['Project Name'] == project_name, 'Longitude'] = lon\n",
    "            df.loc[df['Project Name'] == project_name, 'Full Address'] = address\n",
    "\n",
    "        \n",
    "        # Add delay to avoid hitting API rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    df.to_csv(f'../datasets/updated_coordinates/district{district_num}.csv', index=False)\n",
    "    print(\"Updated dataset saved with coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08f3472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_property_name(name):\n",
    "    # Convert to string in case of non-string input\n",
    "    name = str(name)\n",
    "    \n",
    "    # Remove emojis and special characters\n",
    "    name = re.sub(r'[^\\x00-\\x7F]+', '', name)  # Remove non-ASCII characters\n",
    "    name = re.sub(r'[⭐★☆✨]+', '', name)  # Remove star symbols\n",
    "    \n",
    "    # Remove common advertising phrases\n",
    "    ad_phrases = [\n",
    "        r'!!!.*!!!',\n",
    "        r'CHEAPER.*BAY',\n",
    "        r'LOW ENTRY.*VIEWS',\n",
    "        r'LUXURY LIVING.*',\n",
    "        r'CHEAP.*',\n",
    "        r'UNBLOCK.*VIEW.*',\n",
    "        r'UNDERVALUED.*',\n",
    "        r'Brand New Condos.*',\n",
    "        r'Developer Sale.*',\n",
    "        r'NEW Condo.*',\n",
    "        r'^!!!.*',\n",
    "        r'.*!!!$'\n",
    "    ]\n",
    "    \n",
    "    for phrase in ad_phrases:\n",
    "        name = re.sub(phrase, '', name, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove leading/trailing special characters and whitespace\n",
    "    name = re.sub(r'^[-!@#$%^&*(),.?\":{}|<> ]+|[-!@#$%^&*(),.?\":{}|<> ]+$', '', name)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    # If name becomes empty after cleaning, return None\n",
    "    if not name.strip():\n",
    "        return None\n",
    "        \n",
    "    return name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates for each current listing district dataset\n",
    "for district_num in range(1, 29):\n",
    "    # Read the existing dataset\n",
    "    df = pd.read_csv(f'../scrapers/data/Sep2025/condo-sales-{district_num}-Sep2025.csv', encoding='latin1')\n",
    "\n",
    "    # Create new columns for coordinates\n",
    "    df['Latitude'] = None\n",
    "    df['Longitude'] = None\n",
    "    df['Full Address'] = None\n",
    "\n",
    "    # Process each unique project name\n",
    "    for project_name in df['PropertyName'].unique():\n",
    "        project_name = clean_property_name(project_name)\n",
    "        if project_name is None:\n",
    "            continue\n",
    "        print(f\"Processing: {project_name}\")\n",
    "        lat, lon, address = get_coordinates(project_name)\n",
    "        \n",
    "        # Update all rows with this project name\n",
    "        if lat is not None and lon is not None:\n",
    "            df.loc[df['PropertyName'] == project_name, 'Latitude'] = lat\n",
    "            df.loc[df['PropertyName'] == project_name, 'Longitude'] = lon\n",
    "            df.loc[df['PropertyName'] == project_name, 'Full Address'] = address\n",
    "\n",
    "        # Add delay to avoid hitting API rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    df.to_csv(f'../scrapers/data/Sep2025/updated_coordinates/district{district_num}_current_listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_mrt_stations(project_name, latitude, longitude):\n",
    "    \"\"\"Get coordinates from OneMap API for a given project name\"\"\"\n",
    "    base_url = \"https://www.onemap.gov.sg/api/public/nearbysvc/getNearestMrtStops\"\n",
    "    headers = {\"Authorization\": \"Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo4OTgyLCJmb3JldmVyIjpmYWxzZSwiaXNzIjoiT25lTWFwIiwiaWF0IjoxNzU4NzA5MDY0LCJuYmYiOjE3NTg3MDkwNjQsImV4cCI6MTc1ODk2ODI2NCwianRpIjoiZDJiMGI0ZDktZjUwMi00NzlkLTg1MGQtNjMxZThkNTg1YWE0In0.Zx3mwVumXn7b06tPDpViZEwM_UPV3vH57T_F85v0RZL9bU3Pkr1SeHp2U2E0mgzWeRSO5e-lfT2PmHvw5Abn8E-X3V0brG5Ke9QJNjsaaKocOQuTXKoabS4_X2-GN7GkGPPr5-IFR5braFhTHzZfFmC2vwDwEP6IDYkURV8NuzjmT8yLX29l_gVkiQPxI1_3MPYahDT0sb1IXTRjAmP6R6RGVtVfnQTI1splQxcfNZguY3u4l441caafnoJo101kcFaXLAKo4d2V0EqoN1aKph92wHjbIkFioF-0d_8JGLgshMuHSm2KsI1IpruMnR-x7M1bKkYFSybszg7KuszN_A\"}\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Parameters for the API call\n",
    "        params = {\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'radius_in_meters': '1000'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        stations = []\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                # Extract station names and distances\n",
    "                stations = [station['name'] for station in data]\n",
    "                return \"; \".join(stations)\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        return f\"API Error: {response.status_code}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {project_name}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for district_num in range(15, 29):\n",
    "    try:        \n",
    "        # Read the updated dataset\n",
    "        df = pd.read_csv(f'../datasets/updated_coordinates/district{district_num}.csv')\n",
    "\n",
    "        # Filter out rows without coordinates\n",
    "        df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "        # Create new column for MRT stations if it doesn't exist\n",
    "        if 'Nearest MRT Stations' not in df.columns:\n",
    "            df['Nearest MRT Stations'] = None\n",
    "\n",
    "        # Process each unique project\n",
    "        for project_name in df['Project Name'].unique():\n",
    "            try:\n",
    "                # Get the first row for this project (assuming coordinates are same for same project)\n",
    "                project_row = df[df['Project Name'] == project_name].iloc[0]\n",
    "                \n",
    "                print(f\"Processing: {project_name}\")\n",
    "                                \n",
    "                # Get nearest MRT stations\n",
    "                mrt_stations = get_nearest_mrt_stations(\n",
    "                    project_name,\n",
    "                    str(project_row['Latitude']),  # Convert to string\n",
    "                    str(project_row['Longitude'])  # Convert to string\n",
    "                )\n",
    "                \n",
    "                # Update all rows for this project\n",
    "                df.loc[df['Project Name'] == project_name, 'Nearest MRT Stations'] = mrt_stations\n",
    "                \n",
    "                # Add delay to avoid hitting API rate limits\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {project_name} in district {district_num}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Save the updated dataset\n",
    "        df.to_csv(f'../datasets/updated_coordinates/district{district_num}.csv', index=False)\n",
    "        print(f\"Completed district {district_num}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing district {district_num}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4b3ca",
   "metadata": {},
   "source": [
    "## Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = pd.read_csv('../datasets/supplementary_datasets/hospitals.csv')\n",
    "\n",
    "for idx, row in hospital_df.iterrows():\n",
    "    hospital_name = row['Hospital Name']\n",
    "    print(f\"Processing: {hospital_name}\")\n",
    "    \n",
    "    lat, lon, address = get_coordinates(hospital_name)\n",
    "    \n",
    "    # Update the current row\n",
    "    hospital_df.at[idx, 'Latitude'] = lat\n",
    "    hospital_df.at[idx, 'Longitude'] = lon\n",
    "    hospital_df.at[idx, 'Full Address'] = address\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4227cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df\n",
    "hospital_df.to_csv('../datasets/supplementary_datasets/hospitals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nearest Hospitals in 1km radius\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b6a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
